
    \chapter{Literature Reviews}

\section{Accreditation Process}
In Vietnam, higher education accreditation plays a vital role in maintaining and improving educational quality. The Center for Education Accreditation – Vietnam National University Ho Chi Minh City (CEA VNU-HCM) manages and guides this process in accordance with regulations such as Circular 04.

A typical accreditation project at CEA VNU-HCM usually involves:
\begin{itemize}
    \item A secretary (from the center)
    \item A team of inspectors, usually at least five
    \item An admin or project head who monitors the overall process
\end{itemize}

The process follows a structured sequence of templates and review steps:

\begin{enumerate}
\label{sec:docdefs}
    \item \textbf{Evaluation Plan\label{doc:eval-plan}:} Drafted by the secretary to outline the scope, timeline, and responsibilities, and shared with inspectors and the institution.
    \item \textbf{Individual Evaluation\label{doc:indi-eval}:} Completed separately by each inspector to provide detailed comments on the institution’s self-evaluation report.
    \item \textbf{Consolidated Comments\label{doc:comments}:} Prepared by the secretary by merging all \textbf{Individual Evaluation} inputs into a single comprehensive document and then sent to the institution.
    \item \textbf{Preliminary Survey Report\label{doc:indi-eval}:} Compiled based on \textbf{Consolidated Comments} and site observations, providing early feedback to the institution.
    \item \textbf{Detailed Evaluation of Criteria\label{doc:criteria}:} Each inspector prepares in-depth analyses, citing evidence for each accreditation criterion.
    \item \textbf{Summary Report\label{doc:summary}:} Inspectors summarize findings into a concise narrative based on \textbf{Detailed Evaluation of Criteria}.
    \item \textbf{Official Completion Minutes\label{doc:official}:} The secretary merges all \textbf{Summary Report} summaries into a final consolidated document for the institution.
    \item \textbf{Draft External Assessment Report\label{doc:assest-report}:} The secretary compiles the final draft report based on detailed evaluations and summaries.
\end{enumerate}

While this structured process ensures transparency and standardization, it also introduces complexity, particularly when multiple inspectors and secretaries must collaboratively draft, merge, and refine large volumes of narrative text. Documents often contain repeated content, inconsistent formatting, and language errors due to manual editing. Furthermore, differences in writing style or interpretation among inspectors can lead to inconsistencies and contradictions between versions of the same document.

These challenges are further complicated by evolving accreditation regulations and expectations. As standards change over time, secretaries and inspectors must ensure that older reports and drafts remain compliant, often requiring substantial manual rework. Such repetitive editing reduces time available for substantive evaluation and increases the likelihood of oversight.


\section{Use of Large Language Models and AI for Textual Files Analysis}

Large Language Models (LLMs) and Natural Language Processing (NLP) techniques have become increasingly important tools for analyzing and managing complex textual documents. These technologies are capable of parsing unstructured data, identifying structural gaps, summarizing key content, and detecting duplicated or inconsistent information within large document sets.

In recent years, general-purpose LLMs such as ChatGPT by OpenAI, DeepSeek, Gemini, and others have demonstrated significant capabilities in understanding and processing natural language text. These models can generate drafts, refine language, detect inconsistencies, and even reformat documents to comply with specific templates or standards. For example, ChatGPT is often used to draft reports, proofread documents, and summarize lengthy texts, while DeepSeek and Gemini have been applied to multilingual content analysis and structured document generation.

In the context of accreditation reporting, LLMs can help secretaries and inspectors by analyzing draft inputs and generating complete, structured documents tailored to predefined templates. Beyond content generation, these models can also perform automated error-checking—such as identifying duplicated text, detecting grammar issues, and flagging inconsistencies between sections or document versions. This capability significantly reduces repetitive editing tasks and helps maintain consistency, ultimately improving the quality and reliability of final reports.

By integrating LLMs into the accreditation process, institutions can shift human effort from manual formatting and correction toward more substantive review and analysis, thereby enhancing the overall quality assurance process.
\section{Related Works}
Recent research has demonstrated growing interest in applying NLP and large language models (LLMs) to support the analysis and preparation of complex textual documents. These approaches have shown promise in automating tasks such as content extraction, summarization, and consistency checking—particularly useful in environments that require structured documentation.
\subsection{Enhancing Legal Document Analysis with Large Language Models: A Structured Approach to Accuracy, Context Preservation, and Risk Mitigation} 

Recent advancements in natural language processing have significantly shaped the field of legal document analysis. A notable example is the study by Davenport, Enhancing Legal Document Analysis with Large Language Models: A Structured Approach to Accuracy, Context Preservation, and Risk Mitigation\cite{davenport}, published in the Open Journal of Modern Linguistics. This work presented a case study that applied OpenAI’s GPT-4 model to analyze a municipal-school contract agreement, approximately ten pages long and containing typical contractual clauses such as payment terms, liability provisions, and clearly defined party obligations—elements similar in structure and complexity to accreditation reports.

Davenport proposed a structured methodology designed to address common challenges in large document processing, including token limitations, context fragmentation, and summarization fidelity. Through hierarchical segmentation, chain-of-thought prompting, and multi-stage summarization, the study demonstrated that large language models can effectively extract, synthesize, and summarize key contractual elements with accuracy comparable to human analysts.

Beyond technical methodology, the research also addressed ethical concerns inherent in AI-assisted document analysis, such as the risk of AI hallucinations, data confidentiality, and algorithmic bias. These considerations are increasingly relevant as AI tools are deployed in sensitive legal and administrative contexts.

Overall, the study contributes to a growing body of literature highlighting how large language models can transform complex text analysis. It underscores their potential to support professionals in legal and compliance-focused fields by improving accessibility, identifying ambiguities, and enhancing consistency within structured documents like contracts or accreditation reports.

\subsection{Leveraging Large Language Models for Document Analysis and Decision-Making in AI Chatbots}

Another recent literature demonstrates the transformative role of Large Language Models (LLMs) in document analysis and AI chatbot applications, particularly across domains like legal, healthcare, and finance. Studies have explored how LLMs automate information extraction, summarization, and risk analysis, ultimately improving decision-making efficiency and accuracy.

For example, Leveraging Large Language Models for Document Analysis and Decision-Making in AI Chatbots\cite{islam} proposed chatbot systems that integrate key performance indicators (KPIs) and analytics models to support strategic decision-making, allowing non-technical users to interact more effectively with large datasets. Similarly, Islam M. Ibrahim and his partners developed AI tools capable of automatically identifying risk-prone sections in legal documents, achieving high accuracy and reducing manual review time.

In medical education and clinical contexts, research has highlighted LLM-powered chatbots as valuable tools for real-time decision support and information retrieval, improving accessibility and workflow efficiency. Tools like LangChain and Retrieval-Augmented Generation (RAG) architectures have further advanced document analysis by combining semantic search with generative responses, making static documents interactive and context-aware.

Beyond specialized applications, broader reviews and frameworks have examined how LLMs can be integrated into enterprise workflows. These studies emphasize modular pipelines, API-based integrations, and hybrid approaches that blend LLMs with rule-based systems to balance flexibility and domain specificity.

While these advancements highlight the promise of LLMs in document-intensive sectors, the literature also identifies ongoing challenges. These include high computational costs, data privacy concerns, potential algorithmic biases, and the need for domain adaptation. Despite these limitations, the body of research consistently points to LLMs’ capacity to make document analysis smarter, faster, and more reliable—helping organizations turn complex text data into actionable insights.

\section{Significance of AI in Accreditation Reports}
The application of domain-adapted Large Language Models (LLMs) holds particular significance for Vietnamese accreditation reporting and highly structured, template-based writing mandated by regulatory bodies such as CEA VNU-HCM. Advances in AI and LLMs make it possible to automatically parse and understand these templates, generate draft reports, and error-proof documents by detecting duplicated content, language inconsistencies, and formatting issues.

Beyond basic grammar correction, a refined AI system can analyze document semantics to identify contradictions, omissions, or content misaligned with accreditation criteria. This enables inspectors and secretaries to receive targeted recommendations, improving both compliance and clarity.

By integrating template analysis, automated content generation, and intelligent error detection, such AI systems can significantly enhance the efficiency and consistency of accreditation reporting. This approach allows accreditation teams to concentrate on substantive evaluation and interpretation, while routine formatting and error-checking tasks are streamlined—aligning institutional reporting more closely with national standards and best practices.

